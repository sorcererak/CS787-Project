{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport requests\nfrom typing import List, Dict, Tuple\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom functools import lru_cache\n\nimport google.generativeai as genai\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom transformers import logging as hf_logging\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset\n\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nSERPER_API_KEY = user_secrets.get_secret(\"SERPER_API_KEY\")\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\nlogin(token=HF_TOKEN)\n\nmodel_name = \"meta-llama/Llama-3.2-3B-Instruct\"\nDEVICE_MAP = \"auto\"\nllm_name = \"gemini-2.5-flash-lite\"\n\nDATASET_NAME = \"Rowan/hellaswag\"\nSPLIT_NAME = \"validation\"\nNUM_EXAMPLES = 100\nPRINT_FIRST_N_DEBUG = 3\nBASE_SLEEP_BETWEEN_CALLS = 5.0\nWEB_TIMEOUT = 10\nweb_results = 5\n\nPROMPT_MODE = \"instruction\"\nVARIANTS = [\"our_method\"]\n\nassert GEMINI_API_KEY, \"Set GEMINI_API_KEY in Kaggle secrets.\"\ngenai.configure(api_key=GEMINI_API_KEY)\ngemini_model = genai.GenerativeModel(llm_name)\nprint(\"Gemini model initialised.\")\n\nprint(\"Loading local SLM (this can take a bit)...\")\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\nslm_tokenizer = AutoTokenizer.from_pretrained(model_name)\nslm_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=DEVICE_MAP,\n    quantization_config=bnb_config,\n)\nhf_logging.set_verbosity_error()\nslm_tokenizer.pad_token = slm_tokenizer.eos_token\nslm_model.config.pad_token_id = slm_tokenizer.pad_token_id\nprint(\"Local SLM loaded.\")\n\n\ndef call_gemini_with_cooldown(prompt: str, max_retries: int = 3):\n    last_exc = None\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = gemini_model.generate_content(prompt)\n            time.sleep(BASE_SLEEP_BETWEEN_CALLS)\n            return resp\n        except Exception as e:\n            last_exc = e\n            msg = str(e)\n            wait_s = None\n            m1 = re.search(r\"retry in ([0-9.]+)s\", msg, flags=re.I)\n            if m1:\n                wait_s = float(m1.group(1))\n            else:\n                m2 = re.search(r\"seconds:\\s*([0-9]+)\", msg, flags=re.I)\n                if m2:\n                    wait_s = float(m2.group(1))\n            if wait_s is None:\n                wait_s = 60.0\n            print(f\"[Rate limit] Attempt {attempt}/{max_retries} → sleeping {wait_s:.1f}s...\")\n            time.sleep(wait_s)\n    raise RuntimeError(\"Gemini generate_content failed after retries.\") from last_exc\n\n\ndef slm_generate(prompt: str, max_new_tokens: int = 200) -> str:\n    inputs = slm_tokenizer(prompt, return_tensors=\"pt\").to(slm_model.device)\n    out_ids = slm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n    text = slm_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n    return text.strip()\n\n\ndef web_search(query: str, num_results: int = 2):\n    num_results = min(num_results, web_results)\n    results = []\n    if SERPER_API_KEY:\n        url = \"https://google.serper.dev/search\"\n        headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n        payload = {\"q\": query}\n        try:\n            r = requests.post(url, headers=headers, json=payload, timeout=WEB_TIMEOUT)\n            data = r.json()\n            if data.get(\"organic\"):\n                for item in data[\"organic\"][:num_results]:\n                    snippet = (item.get(\"snippet\") or \"\").replace(\"\\n\", \" \")\n                    link = item.get(\"link\") or \"\"\n                    if snippet:\n                        results.append({\"snippet\": snippet, \"url\": link})\n        except Exception as e:\n            print(\"Serper error:\", e)\n    if not results:\n        results.append({\"snippet\": f\"General information about: {query}\", \"url\": \"\"})\n    return results[:num_results]\n\n\n@lru_cache(maxsize=4096)\ndef web_search_cached(q: str, num_results: int = 2):\n    res = web_search(q, num_results=num_results)\n    return tuple((d[\"snippet\"], d[\"url\"]) for d in res)\n\n\ndef _looks_bad(q: str) -> bool:\n    q = (q or \"\").strip().lower()\n    if not q:\n        return True\n    if q in {\"<why>\", \"<what>\", \"<how>\"}:\n        return True\n    if len(re.sub(r\"[^a-z]\", \"\", q)) < 5:\n        return True\n    return False\n\n\ndef _simple_fallback_decomp(full_q: str) -> Dict[str, str]:\n    base = re.sub(r\"\\s*\\?$\", \"\", full_q).strip()\n    if not base:\n        base = full_q.strip()\n    return {\n        \"WHY\": f\"Why is the following context challenging for a commonsense completion task: {base}?\",\n        \"WHAT\": f\"What key real-world facts or commonsense knowledge are needed to complete: {base}?\",\n        \"HOW\": f\"How could everyday experience and physics help decide the most plausible ending for: {base}?\",\n    }\n\n\ndef slm_decompose_queries(full_question: str) -> Dict[str, str]:\n    prompt = (\n        \"You are a query decomposition assistant for HellaSwag-style commonsense \"\n        \"narrative completion tasks.\\n\"\n        \"Given a context and candidate endings, rewrite it into exactly three concise \"\n        \"sub-queries:\\n\"\n        \"WHY: what high-level commonsense reasoning is needed\\n\"\n        \"WHAT: key entities, actions, or world knowledge required\\n\"\n        \"HOW: steps or logical process to compare plausibility of endings\\n\\n\"\n        \"Return EXACTLY:\\nWHY: <text>\\nWHAT: <text>\\nHOW: <text>\\n\\n\"\n        f\"Input (context + instructions): {full_question}\\n\\nDecomposed queries:\"\n    )\n    text = slm_generate(prompt, max_new_tokens=192)\n    why = what = how = \"\"\n    m_why = re.search(r\"WHY:\\s*(.+)\", text, flags=re.I)\n    m_what = re.search(r\"WHAT:\\s*(.+)\", text, flags=re.I)\n    m_how = re.search(r\"HOW:\\s*(.+)\", text, flags=re.I)\n    if m_why:\n        why = m_why.group(1).strip()\n    if m_what:\n        what = m_what.group(1).strip()\n    if m_how:\n        how = m_how.group(1).strip()\n    if _looks_bad(why) or _looks_bad(what) or _looks_bad(how):\n        return _simple_fallback_decomp(full_question)\n    return {\"WHY\": why, \"WHAT\": what, \"HOW\": how}\n\n\ndef slm_build_hints(full_question: str, tagged_snippets: List[Dict[str, str]]) -> List[str]:\n    prompt = (\n        \"You are helping with a HellaSwag-style commonsense reasoning benchmark.\\n\"\n        \"Given the context and instructions, and web snippets tagged WHY/WHAT/HOW,\\n\"\n        \"extract 4–8 short, verifiable bullet points that capture relevant real-world\\n\"\n        \"knowledge (for example typical sequences of actions, physical constraints, social norms).\\n\"\n        \"Do NOT answer the question and do NOT mention any option letter.\\n\\n\"\n        f\"Context and instructions:\\n{full_question}\\n\\nWeb snippets:\\n\"\n    )\n    for i, s in enumerate(tagged_snippets, 1):\n        prompt += f\"[{i}] {s['snippet']}\\n\"\n    prompt += \"\\nWrite 4–8 bullets. Start each line with '- ' only:\\n\"\n    text = slm_generate(prompt, max_new_tokens=256)\n    facts: List[str] = []\n    for line in text.splitlines():\n        ln = line.strip()\n        if re.match(r\"^(-|\\*|•|\\d+\\.)\\s+\", ln):\n            ln = re.sub(r\"^(\\*|•|\\d+\\.)\\s+\", \"- \", ln)\n        if ln.startswith(\"- \"):\n            fact = ln[2:].strip()\n            if fact:\n                facts.append(fact)\n    if not facts:\n        facts = [s[\"snippet\"] for s in tagged_snippets]\n    return facts[:8]\n\n\ndef build_hellaswag_instruction_prompt(\n    context: str,\n    options_dict: Dict[str, str],\n    hints: List[str] = None,\n) -> str:\n    text = (\n        \"You are solving a commonsense narrative completion multiple-choice question \"\n        \"from the HellaSwag benchmark.\\n\"\n        \"You are given a short context and several possible endings.\\n\"\n        \"Your task is to choose the single most plausible, coherent ending that continues \"\n        \"the context in a natural way.\\n\\n\"\n        \"Instructions:\\n\"\n        \"  • Carefully read the context and understand what is happening.\\n\"\n        \"  • Read each candidate ending and check whether it is grammatically correct,\\n\"\n        \"    logically consistent, and likely given the context.\\n\"\n        \"  • Prefer endings that match everyday physical and social commonsense.\\n\"\n        \"  • You may reason step-by-step INTERNALLY, but you MUST NOT show your reasoning.\\n\"\n        \"  • Only output the final answer option as a single capital letter: A, B, C, or D.\\n\\n\"\n        f\"Context:\\n{context}\\n\\n\"\n        \"Candidate endings:\\n\"\n    )\n    for lab in [\"A\", \"B\", \"C\", \"D\"]:\n        text += f\"{lab}. {options_dict[lab]}\\n\"\n    if hints:\n        text += \"\\nOptional factual or world-knowledge hints from a smaller helper model (may be incomplete or noisy):\\n\"\n        for h in hints:\n            text += f\"- {h}\\n\"\n    text += \"\\nFinal answer (ONLY one letter A, B, C, or D):\"\n    return text\n\n\ndef gemini_hellaswag_answer(\n    context: str,\n    options_dict: Dict[str, str],\n    hints: List[str] = None,\n) -> Tuple[str, str]:\n    prompt = build_hellaswag_instruction_prompt(context, options_dict, hints=hints)\n    resp = call_gemini_with_cooldown(prompt)\n    out_text = (getattr(resp, \"text\", \"\") or \"\").strip()\n    m = re.search(r\"\\b[A-D]\\b\", out_text)\n    if m:\n        return m.group(0), out_text\n    return \"A\", out_text\n\n\ndef prepare_hellaswag_example(row) -> Tuple[str, Dict[str, str], str]:\n    context = str(row[\"ctx\"])\n    endings_list = list(row[\"endings\"])\n    if len(endings_list) != 4:\n        endings_list = (endings_list + [\"\"] * 4)[:4]\n    options_dict = {\n        \"A\": str(endings_list[0]),\n        \"B\": str(endings_list[1]),\n        \"C\": str(endings_list[2]),\n        \"D\": str(endings_list[3]),\n    }\n    ans_idx_raw = row[\"label\"]\n    ans_idx = int(ans_idx_raw)\n    gold = \"ABCD\"[ans_idx]\n    return context, options_dict, gold\n\n\ndef predict_hellaswag_for_row(\n    row,\n    variant: str = \"our_method\",\n    verbose: bool = False,\n):\n    context, options_dict, gold = prepare_hellaswag_example(row)\n    full_q = (\n        \"Context: \"\n        + context\n        + \"\\n\\nCandidate endings:\\n\"\n        f\"A. {options_dict['A']}\\n\"\n        f\"B. {options_dict['B']}\\n\"\n        f\"C. {options_dict['C']}\\n\"\n        f\"D. {options_dict['D']}\\n\\n\"\n        \"Question: Which ending is the most plausible, coherent continuation?\"\n    )\n    sub_queries: Dict[str, str] = {}\n    hints: List[str] = []\n    if variant == \"our_method\":\n        sub_queries = slm_decompose_queries(full_q)\n        tagged = []\n        for tag in [\"WHY\", \"WHAT\", \"HOW\"]:\n            q_sub = sub_queries[tag]\n            for snip, url in web_search_cached(q_sub, num_results=2):\n                tagged.append({\"snippet\": f\"[{tag}] {snip}\", \"url\": url})\n        hints = slm_build_hints(full_q, tagged)\n    pred_label, gemini_raw = gemini_hellaswag_answer(\n        context,\n        options_dict,\n        hints=hints if hints else None,\n    )\n    if verbose:\n        print(\"\\n\" + \"=\" * 70)\n        print(f\"Variant   : {variant}\")\n        print(f\"Context   :\\n{context[:600]}...\\n\")\n        print(\"Candidate endings:\")\n        for lab in [\"A\", \"B\", \"C\", \"D\"]:\n            print(f\"  {lab}. {options_dict[lab]}\")\n        print(f\"\\nGold label: {gold}\")\n        print(f\"Pred label: {pred_label}\")\n        if variant == \"our_method\":\n            print(\"\\nSub-queries:\")\n            for k, v in sub_queries.items():\n                print(f\"  {k}: {v}\")\n            print(\"\\nSample hints:\")\n            for h in hints[:5]:\n                print(f\"  - {h}\")\n    return pred_label, gold, sub_queries, hints, gemini_raw\n\n\ndataset = load_dataset(DATASET_NAME)\nds_split = dataset[SPLIT_NAME]\ndf_all = ds_split.to_pandas()\ndf_split = df_all.iloc[:NUM_EXAMPLES].reset_index(drop=True)\n\nprint(f\"HellaSwag split '{SPLIT_NAME}' loaded — using first {len(df_split)} examples.\")\nprint(\"Columns:\", list(df_split.columns))\n\nrows_out: List[Dict] = []\npreds: List[str] = []\ngolds: List[str] = []\ncorrect_flags: List[int] = []\n\nfor variant in VARIANTS:\n    print(f\"\\n=== Running HellaSwag — variant={variant}, prompt_mode={PROMPT_MODE} ===\")\n    for i in tqdm(range(len(df_split)), desc=variant):\n        r = df_split.iloc[i]\n        pred_label, gold, sub_queries, hints, gemini_raw = predict_hellaswag_for_row(\n            r,\n            variant=variant,\n            verbose=(i < PRINT_FIRST_N_DEBUG),\n        )\n        preds.append(pred_label)\n        golds.append(gold)\n        correct = int(pred_label == gold)\n        correct_flags.append(correct)\n        endings_list = list(r[\"endings\"])\n        if len(endings_list) != 4:\n            endings_list = (endings_list + [\"\"] * 4)[:4]\n        rows_out.append({\n            \"idx_in_split\": int(i),\n            \"variant\": variant,\n            \"split\": SPLIT_NAME,\n            \"prompt_mode\": PROMPT_MODE,\n            \"context\": r[\"ctx\"],\n            \"ending0\": str(endings_list[0]),\n            \"ending1\": str(endings_list[1]),\n            \"ending2\": str(endings_list[2]),\n            \"ending3\": str(endings_list[3]),\n            \"gold_label\": gold,\n            \"pred_label\": pred_label,\n            \"correct\": correct,\n            \"gemini_output\": gemini_raw,\n            \"sub_queries\": json.dumps(sub_queries, ensure_ascii=False) if sub_queries else \"\",\n            \"hints\": json.dumps(hints, ensure_ascii=False) if hints else \"\",\n        })\n        if i >= PRINT_FIRST_N_DEBUG:\n            print(f\"Example {i+1}: gold={gold} | pred={pred_label} | correct={bool(correct)}\")\n\nacc = sum(correct_flags) / max(1, len(preds))\nprint(f\"\\nVariant={VARIANTS[0]}, mode={PROMPT_MODE} — accuracy: {acc:.3f} on {len(preds)} examples.\")\n\ndf_out = pd.DataFrame(rows_out)\nout_path = f\"hellaswag_{SPLIT_NAME}_instruction_our_method.csv\"\ndf_out.to_csv(out_path, index=False)\nprint(\"\\nSaved:\", out_path)\n\nprint(\"\\nSummary (accuracy by variant):\")\nfor variant in VARIANTS:\n    variant_preds = [row[\"pred_label\"] for row in rows_out if row[\"variant\"] == variant]\n    variant_golds = [row[\"gold_label\"] for row in rows_out if row[\"variant\"] == variant]\n    if not variant_preds:\n        continue\n    acc = sum(int(p == g) for p, g in zip(variant_preds, variant_golds)) / max(1, len(variant_preds))\n    print(f\"  {variant:10s} — accuracy {acc:.3f} on {len(variant_preds)} examples\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}