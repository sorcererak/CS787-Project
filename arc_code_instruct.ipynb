{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport google.generativeai as genai\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HF_TOKEN\"))\n\nSERPER_API_KEY = user_secrets.get_secret(\"SERPER_API_KEY\")\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\npath = \"/kaggle/input/arc-challenge/ai2_arc_100.csv\"\n\nmodel_name = \"...\"\nDEVICE_MAP = \"auto\"\nllm_name = \"gemini-2.5-flash-lite\"\nweb_results = 5\nweb_timeout = 10\n\ntotal_eval_datapoints = 100\nSLEEP_BETWEEN_GEMINI_CALLS = 5\nGEMINI_MAX_RETRIES = 3\n\ngenai.configure(api_key=GEMINI_API_KEY)\ngenai_model = genai.GenerativeModel(llm_name)\nprint(\"Gemini model initialised.\")\n\nprint(\"Loading local SLM (this can take a bit)...\")\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\nslm_tokenizer = AutoTokenizer.from_pretrained(model_name)\nslm_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=DEVICE_MAP,\n    quantization_config=bnb_config,\n)\nprint(\"Local SLM loaded.\")\n\n\ndef slm_generate(prompt: str, max_new_tokens: int = 64) -> str:\n    inputs = slm_tokenizer(prompt, return_tensors=\"pt\").to(slm_model.device)\n    out_ids = slm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n    text = slm_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n    return text.strip()\n\n\ndef web_search(query: str, num_results: int = 2):\n    num_results = min(num_results, web_results)\n    results = []\n\n    if SERPER_API_KEY:\n        url = \"https://google.serper.dev/search\"\n        headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n        payload = {\"q\": query}\n        try:\n            r = requests.post(url, headers=headers, json=payload, timeout=web_timeout)\n            data = r.json()\n            if data.get(\"organic\"):\n                for item in data[\"organic\"][:num_results]:\n                    snippet = (item.get(\"snippet\") or \"\").replace(\"\\n\", \" \")\n                    link = item.get(\"link\") or \"\"\n                    if snippet:\n                        results.append({\"snippet\": snippet, \"url\": link})\n        except Exception as e:\n            print(\"Serper error:\", e)\n\n    if not results:\n        results.append({\"snippet\": f\"General information about: {query}\", \"url\": \"\"})\n\n    return results[:num_results]\n\n\ndef call_gemini_with_cooldown(prompt: str, max_retries: int = GEMINI_MAX_RETRIES):\n    if genai_model is None:\n        raise RuntimeError(\"Gemini model not initialised.\")\n\n    last_exc = None\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = genai_model.generate_content(prompt)\n            time.sleep(SLEEP_BETWEEN_GEMINI_CALLS)\n            return resp\n        except Exception as e:\n            last_exc = e\n            msg = str(e)\n            wait_s = None\n            m1 = re.search(r\"retry in ([0-9.]+)s\", msg, flags=re.I)\n            if m1:\n                wait_s = float(m1.group(1))\n            else:\n                m2 = re.search(r\"seconds:\\s*([0-9]+)\", msg, flags=re.I)\n                if m2:\n                    wait_s = float(m2.group(1))\n            if wait_s is None:\n                wait_s = 60.0\n            print(f\"[Gemini rate/HTTP error] Attempt {attempt}/{max_retries} â†’ sleeping {wait_s:.1f}s...\")\n            time.sleep(wait_s)\n\n    raise RuntimeError(f\"Gemini generate_content failed after {max_retries} retries.\") from last_exc\n\n\ndef slm_decompose_queries(question: str):\n    prompt = (\n        \"You are a query decomposition assistant for science exam questions.\\n\"\n        \"Given a multiple-choice question, rewrite it into exactly three concise\\n\"\n        \"sub-questions:\\n\"\n        \"1) a 'WHY' question\\n\"\n        \"2) a 'WHAT' question\\n\"\n        \"3) a 'HOW' question\\n\\n\"\n        \"Return them in the following format exactly:\\n\"\n        \"WHY: <why_question>\\n\"\n        \"WHAT: <what_question>\\n\"\n        \"HOW: <how_question>\\n\\n\"\n        f\"Original question: {question}\\n\\n\"\n        \"Decomposed queries:\"\n    )\n\n    text = slm_generate(prompt, max_new_tokens=128)\n\n    why = what = how = question\n    m_why = re.search(r\"WHY:\\s*(.+)\", text, flags=re.IGNORECASE)\n    m_what = re.search(r\"WHAT:\\s*(.+)\", text, flags=re.IGNORECASE)\n    m_how = re.search(r\"HOW:\\s*(.+)\", text, flags=re.IGNORECASE)\n\n    if m_why:\n        why = m_why.group(1).strip()\n    if m_what:\n        what = m_what.group(1).strip()\n    if m_how:\n        how = m_how.group(1).strip()\n\n    return {\"WHY\": why, \"WHAT\": what, \"HOW\": how}\n\n\ndef slm_build_hints(question: str, tagged_snippets):\n    prompt = (\n        \"You are given a science question and some web snippets that were retrieved\\n\"\n        \"using different sub-queries (WHY / WHAT / HOW).\\n\"\n        \"Your job is to extract 4-8 short factual points that are helpful for answering the question.\\n\"\n        \"Write them as an enumerated list using i), ii), iii), iv), ...\\n\"\n        \"Each point must be on its own line starting with i), ii), etc.\\n\"\n        \"IMPORTANT: Do NOT answer the question yourself. Only write the factual points.\\n\\n\"\n        f\"Question: {question}\\n\\n\"\n        \"Web snippets (with tags):\\n\"\n    )\n    for i, s in enumerate(tagged_snippets, 1):\n        prompt += f\"[{i}] {s['snippet']}\\n\"\n\n    prompt += (\n        \"\\nNow write 4-8 factual points, each on a new line, starting with i), ii), iii), etc.\\n\"\n        \"Do not include anything else.\\n\"\n    )\n\n    text = slm_generate(prompt, max_new_tokens=192)\n\n    facts = []\n    for line in text.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        m = re.match(r\"(?i)^([ivx]+)\\)\\s*(.+)\", line)\n        if m:\n            fact = m.group(2).strip()\n        elif line.startswith(\"-\"):\n            fact = line.lstrip(\"-\").strip()\n        else:\n            continue\n        if fact:\n            facts.append(fact)\n\n    if not facts:\n        facts = [s[\"snippet\"] for s in tagged_snippets]\n\n    return facts\n\n\ndef build_instruction_prompt(question, options_dict, facts):\n    text = (\n        \"You are a highly capable science exam solver.\\n\"\n        \"Your goal is to choose the single best answer to the multiple-choice question.\\n\\n\"\n        \"You are given:\\n\"\n        \"  i) the question\\n\"\n        \"  ii) four answer options (A, B, C, D)\\n\"\n        \"  iii) OPTIONAL factual hints produced by a smaller helper model from web search\\n\\n\"\n        \"Use the following strategy:\\n\"\n        \"  1. Carefully read the question and options.\\n\"\n        \"  2. Use your own scientific knowledge and reasoning to understand what is being asked.\\n\"\n        \"  3. Look at the hints only as supplemental context: they may be helpful, but they can also\\n\"\n        \"     be incomplete, noisy, or partially irrelevant.\\n\"\n        \"  4. If the hints are consistent with the question and your own reasoning, you may use them\\n\"\n        \"     to support or refine your choice. If the hints conflict with the question or seem\\n\"\n        \"     unhelpful, ignore them and answer independently.\\n\"\n        \"  5. Silently eliminate clearly wrong options and select the single best remaining option.\\n\\n\"\n        \"Important formatting instruction:\\n\"\n        \"  i) Think through the problem internally, but DO NOT show your reasoning.\\n\"\n        \"  ii) Return ONLY a single capital letter: A, B, C, or D, with no explanation.\\n\\n\"\n        f\"Question: {question}\\n\\n\"\n        \"Options:\\n\"\n        f\"A. {options_dict['A']}\\n\"\n        f\"B. {options_dict['B']}\\n\"\n        f\"C. {options_dict['C']}\\n\"\n        f\"D. {options_dict['D']}\\n\\n\"\n    )\n\n    if facts:\n        text += \"Optional factual hints (from a smaller helper model):\\n\"\n        for f in facts:\n            text += f\"i) {f}\\n\"\n    else:\n        text += (\n            \"Optional factual hints:\\n\"\n            \"i) (none provided; rely on your own knowledge)\\n\"\n        )\n\n    text += \"\\nAnswer (ONLY one letter A, B, C, or D):\"\n    return text\n\n\ndef gemini_mcq_answer(question, options_dict, facts):\n    if genai_model is None:\n        return \"A\"\n\n    text = build_instruction_prompt(question, options_dict, facts)\n\n    try:\n        resp = call_gemini_with_cooldown(text)\n        out_text = resp.text.strip()\n    except Exception as e:\n        print(\"Gemini error while answering MCQ after retries:\", e)\n        out_text = \"\"\n\n    m = re.search(r\"[ABCD]\", out_text)\n    if m:\n        return m.group(0)\n    return \"A\"\n\n\ndef predict_mcq_for_row(row, q_col, opt_cols, verbose: bool = False):\n    question = str(row[q_col])\n    options = {\n        \"A\": str(row[opt_cols[0]]),\n        \"B\": str(row[opt_cols[1]]),\n        \"C\": str(row[opt_cols[2]]),\n        \"D\": str(row[opt_cols[3]]),\n    }\n\n    sub_queries = slm_decompose_queries(question)\n    tagged_snippets = []\n\n    for tag in [\"WHY\", \"WHAT\", \"HOW\"]:\n        subq = sub_queries[tag]\n        sub_snips = web_search(subq, num_results=2)\n        for s in sub_snips:\n            tagged_snippets.append(\n                {\n                    \"snippet\": f\"[{tag}] {s['snippet']}\",\n                    \"url\": s[\"url\"],\n                }\n            )\n\n    facts = slm_build_hints(question, tagged_snippets)\n    ans_llm = gemini_mcq_answer(question, options, facts)\n    final_pred = ans_llm\n\n    if verbose:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"Variant: our_method | Prompt: instruction\")\n        print(f\"Question: {question}\\n\")\n        print(\"Options:\")\n        for lab in [\"A\", \"B\", \"C\", \"D\"]:\n            print(f\"{lab}. {options[lab]}\")\n        print(\"\\nDecomposed sub-queries:\")\n        for tag in [\"WHY\", \"WHAT\", \"HOW\"]:\n            print(f\"  {tag}: {sub_queries[tag]}\")\n        print(\"\\nTagged web snippets:\")\n        for i, s in enumerate(tagged_snippets, 1):\n            print(f\"  [{i}] {s['snippet'][:160]}\")\n        print(\"\\nSLM factual points (i), ii), ... style):\")\n        for f in facts:\n            print(f\"  - {f}\")\n        print(\"\\nLLM (Gemini) final answer:\", final_pred)\n\n    return final_pred, ans_llm, sub_queries, facts\n\n\ndf = pd.read_csv(path)\ncols = list(df.columns)\nq_col = cols[0]\nopt_cols = cols[1:5]\nans_col = cols[5]\n\nprint(\"Columns:\", cols)\nprint(\"Question column:\", q_col)\nprint(\"Option columns:\", opt_cols)\nprint(\"Answer column:\", ans_col)\n\ntotal = min(total_eval_datapoints, len(df))\n\npredictions = []\ngold_labels = []\ncorrect_flags = []\nllm_answers = []\nall_sub_queries = []\nall_facts = []\n\nfor idx in tqdm(range(total), desc=\"ARC-100 (our_method, instruction)\"):\n    row = df.iloc[idx]\n    gold = str(row[ans_col]).strip().upper()\n    gold_labels.append(gold)\n\n    verbose = idx < PRINT_FIRST_N_DEBUG\n\n    pred, ans_llm, sub_queries, facts = predict_mcq_for_row(\n        row,\n        q_col,\n        opt_cols,\n        verbose=verbose,\n    )\n\n    predictions.append(pred)\n    llm_answers.append(ans_llm)\n    all_sub_queries.append(sub_queries)\n    all_facts.append(facts)\n\n    is_corr = int(pred == gold)\n    correct_flags.append(is_corr)\n\n    if not verbose:\n        print(f\"Q{idx + 1}: gold={gold}, pred={pred}, LLM={ans_llm}\")\n\ncorrect = sum(correct_flags)\naccuracy = correct / total\nprint(f\"\\nFinished {total} questions.\")\nprint(f\"Pipeline accuracy (our_method, instruction): {accuracy:.3f}\")\n\neval_df = df.iloc[:total].copy()\neval_df[\"gold\"] = gold_labels\neval_df[\"pred\"] = predictions\neval_df[\"correct\"] = correct_flags\neval_df[\"llm_answer\"] = llm_answers\neval_df[\"sub_queries\"] = [json.dumps(q) for q in all_sub_queries]\neval_df[\"facts\"] = [json.dumps(f) for f in all_facts]\n\nsave_path = \"arc_instruction.csv\"\neval_df.to_csv(save_path, index=False)\nprint(f\"Saved detailed results to: {save_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}