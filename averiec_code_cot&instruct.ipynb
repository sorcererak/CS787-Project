{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport random\nfrom typing import List, Dict, Tuple\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport google.generativeai as genai\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset\n\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\nassert HF_TOKEN, \"Set HF_TOKEN in Kaggle secrets.\"\nassert GEMINI_API_KEY, \"Set GEMINI_API_KEY in Kaggle secrets.\"\n\nlogin(token=HF_TOKEN)\n\nllm_name = \"gemini-2.5-flash-lite\"\n\nNUM_CLAIMS = 100\nPRINT_FIRST_N_DEBUG = 3\nBASE_SLEEP_BETWEEN_CALLS = 5.0\nGEMINI_MAX_RETRIES = 3\n\nPROMPT_MODES = [\"instruction\", \"cot\"]\nVARIANTS = [\"our_method\"]\n\nCANONICAL_LABELS = [\n    \"Supported\",\n    \"Refuted\",\n    \"Not Enough Evidence\",\n    \"Conflicting Evidence/Cherrypicking\",\n]\n\ngenai.configure(api_key=GEMINI_API_KEY)\ngemini_model = genai.GenerativeModel(llm_name)\nprint(\"Gemini model initialised for AVeriTeC.\")\n\n\ndef call_gemini_with_cooldown(prompt: str, max_retries: int = GEMINI_MAX_RETRIES):\n    last_exc = None\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = gemini_model.generate_content(prompt)\n            time.sleep(BASE_SLEEP_BETWEEN_CALLS)\n            return resp\n        except Exception as e:\n            last_exc = e\n            msg = str(e)\n            wait_s = None\n            m1 = re.search(r\"retry in ([0-9.]+)s\", msg, flags=re.I)\n            if m1:\n                wait_s = float(m1.group(1))\n            else:\n                m2 = re.search(r\"seconds:\\s*([0-9]+)\", msg, flags=re.I)\n                if m2:\n                    wait_s = float(m2.group(1))\n            if wait_s is None:\n                wait_s = 60.0\n            print(f\"[Gemini rate/HTTP error] Attempt {attempt}/{max_retries} → sleeping {wait_s:.1f}s...\")\n            time.sleep(wait_s)\n    raise RuntimeError(f\"Gemini generate_content failed after {max_retries} retries.\") from last_exc\n\n\ndef _normalize_label_token(raw: str) -> str:\n    if not raw:\n        return \"Not Enough Evidence\"\n    raw = raw.strip()\n    raw = re.sub(r\"[.\\s]+$\", \"\", raw)\n    low = raw.lower()\n    if low in {\"1\", \"supported\"}:\n        return \"Supported\"\n    if low in {\"2\", \"refuted\"}:\n        return \"Refuted\"\n    if low in {\"3\"} or \"not enough evidence\" in low or \"insufficient evidence\" in low:\n        return \"Not Enough Evidence\"\n    if low in {\"4\"} or \"conflicting\" in low or \"cherry\" in low:\n        return \"Conflicting Evidence/Cherrypicking\"\n    for lab in CANONICAL_LABELS:\n        if lab.lower() == low:\n            return lab\n    for lab in CANONICAL_LABELS:\n        if lab.lower() in low:\n            return lab\n    return \"Not Enough Evidence\"\n\n\ndef extract_label_from_output(text: str) -> str:\n    if not text:\n        return \"Not Enough Evidence\"\n    m = re.search(r\"Final label:\\s*([^\\n]+)\", text, flags=re.I)\n    if m:\n        candidate = m.group(1).strip()\n        return _normalize_label_token(candidate)\n    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]\n    if not lines:\n        return \"Not Enough Evidence\"\n    candidate = lines[-1]\n    m2 = re.search(r\"Label:\\s*([^\\n]+)\", candidate, flags=re.I)\n    if m2:\n        candidate = m2.group(1).strip()\n    return _normalize_label_token(candidate)\n\n\ndef _build_averitec_base_desc() -> str:\n    return (\n        \"You are a careful fact-checking assistant for real-world political and news claims.\\n\"\n        \"Your task is to assign EXACTLY ONE veracity label to the claim, choosing from:\\n\"\n        \"1. Supported\\n\"\n        \"2. Refuted\\n\"\n        \"3. Not Enough Evidence\\n\"\n        \"4. Conflicting Evidence/Cherrypicking\\n\\n\"\n        \"Definitions:\\n\"\n        \"i) Supported: The claim is true or well-supported by reliable evidence.\\n\"\n        \"ii) Refuted: The claim is false or clearly contradicted by reliable evidence.\\n\"\n        \"iii) Not Enough Evidence: Available information is insufficient or inconclusive to decide.\\n\"\n        \"iv) Conflicting Evidence/Cherrypicking: There is both supporting and refuting evidence,\\n\"\n        \"  or the claim selectively highlights only part of the evidence (cherry-picking).\\n\\n\"\n    )\n\n\ndef _build_hints_block(hints: List[str]) -> str:\n    if not hints:\n        return \"\"\n    text = (\n        \"You are also given high-quality bullet-point hints that summarise key factual\\n\"\n        \"evidence collected about this claim (they may still be incomplete, but are usually\\n\"\n        \"very informative). Give these hints significant weight when deciding the label.\\n\\n\"\n        \"Hints:\\n\"\n    )\n    for h in hints:\n        text += f\"- {h}\\n\"\n    text += \"\\n\"\n    return text\n\n\ndef build_averitec_instruction_prompt(claim: str, hints: List[str] = None) -> str:\n    desc = _build_averitec_base_desc()\n    hints_block = _build_hints_block(hints or [])\n    final_instr = (\n        \"You may reason internally, but DO NOT show your reasoning.\\n\"\n        \"Respond with exactly one line in the format:\\n\"\n        \"Final label: <one of Supported / Refuted / Not Enough Evidence / Conflicting Evidence/Cherrypicking>\\n\"\n    )\n    prompt = (\n        desc\n        + hints_block\n        + \"Claim to label:\\n\"\n        + claim\n        + \"\\n\\n\"\n        + final_instr\n    )\n    return prompt\n\n\ndef build_averitec_cot_prompt(claim: str, hints: List[str] = None) -> str:\n    desc = _build_averitec_base_desc()\n    hints_block = _build_hints_block(hints or [])\n    final_instr = (\n        \"Think step by step about whether the claim is supported, refuted, lacks evidence,\\n\"\n        \"or has conflicting evidence. Briefly explain your reasoning.\\n\"\n        \"Then, on a new line, write exactly:\\n\"\n        \"Final label: <one of Supported / Refuted / Not Enough Evidence / Conflicting Evidence/Cherrypicking>\\n\"\n    )\n    prompt = (\n        desc\n        + hints_block\n        + \"Claim to label:\\n\"\n        + claim\n        + \"\\n\\n\"\n        + final_instr\n    )\n    return prompt\n\n\ndef gemini_veracity_answer(\n    claim: str,\n    hints: List[str] = None,\n    mode: str = \"instruction\",\n) -> Tuple[str, str]:\n    mode = mode.lower()\n    if mode == \"cot\":\n        prompt = build_averitec_cot_prompt(claim, hints=hints)\n    else:\n        prompt = build_averitec_instruction_prompt(claim, hints=hints)\n    resp = call_gemini_with_cooldown(prompt)\n    out_text = (getattr(resp, \"text\", \"\") or \"\").strip()\n    label_pred = extract_label_from_output(out_text)\n    return label_pred, out_text\n\n\ndef decompositions_to_hints(decomp_field) -> List[str]:\n    hints: List[str] = []\n    if isinstance(decomp_field, list):\n        for x in decomp_field:\n            if not isinstance(x, str):\n                continue\n            s = x.strip()\n            if not s:\n                continue\n            hints.append(s)\n    elif isinstance(decomp_field, str):\n        for part in re.split(r\"[\\n;]+\", decomp_field):\n            s = part.strip()\n            if s:\n                hints.append(s)\n    return hints[:8]\n\n\ndef predict_claim_for_row(\n    row,\n    variant: str = \"our_method\",\n    mode: str = \"instruction\",\n    verbose: bool = False,\n):\n    claim = str(row[\"claim\"])\n    gold = str(row[\"label\"]).strip()\n    hints: List[str] = []\n    if variant == \"our_method\":\n        hints = decompositions_to_hints(row.get(\"decomposition\", []))\n    label_pred, gemini_raw = gemini_veracity_answer(\n        claim,\n        hints=hints if hints else None,\n        mode=mode,\n    )\n    if verbose:\n        print(\"\\n\" + \"=\" * 70)\n        print(f\"Variant      : {variant}\")\n        print(f\"Prompt mode  : {mode}\")\n        print(f\"Claim        : {claim}\")\n        print(f\"Gold label   : {gold}\")\n        print(f\"Pred label   : {label_pred}\")\n        if variant == \"our_method\":\n            print(\"\\nHints from decomposition (first few):\")\n            for h in hints[:5]:\n                print(f\"  - {h}\")\n    return label_pred, gemini_raw, hints\n\n\ndataset = load_dataset(\"AlbertHatsuki/AveriTeC-decomposed\")\ndf_all = dataset[\"train\"].to_pandas()\n\nassert all(col in df_all.columns for col in [\"id\", \"claim\", \"label\", \"decomposition\"]), \\\n    f\"Unexpected columns: {list(df_all.columns)}\"\n\nsub_df = df_all.iloc[:NUM_CLAIMS].reset_index(drop=True)\nprint(f\"Loaded AVeriTeC-decomposed train split — using first {len(sub_df)} claims.\")\n\nrows_out = []\n\nfor mode in PROMPT_MODES:\n    print(f\"\\n================ PROMPT_MODE = {mode} ================\")\n    for variant in VARIANTS:\n        preds = []\n        golds = []\n        print(f\"\\n=== Running AVeriTeC — variant={variant}, mode={mode} ===\")\n        for i in tqdm(range(len(sub_df)), desc=f\"{variant}-{mode}\"):\n            r = sub_df.iloc[i]\n            gold = str(r[\"label\"]).strip()\n            label_pred, gemini_raw, hints = predict_claim_for_row(\n                r,\n                variant=variant,\n                mode=mode,\n                verbose=(i < PRINT_FIRST_N_DEBUG),\n            )\n            preds.append(label_pred)\n            golds.append(gold)\n            correct = int(label_pred == gold)\n            rows_out.append({\n                \"id\": int(r[\"id\"]),\n                \"variant\": variant,\n                \"prompt_mode\": mode,\n                \"claim\": r[\"claim\"],\n                \"label_gold\": gold,\n                \"label_pred\": label_pred,\n                \"correct\": correct,\n                \"gemini_output\": gemini_raw,\n                \"hints_used\": json.dumps(hints, ensure_ascii=False) if hints else \"\",\n            })\n            if i >= PRINT_FIRST_N_DEBUG:\n                print(f\"Claim {i+1}: gold={gold} | pred={label_pred} | correct={bool(correct)}\")\n        acc = sum(int(p == g) for p, g in zip(preds, golds)) / max(1, len(preds))\n        print(f\"\\nVariant={variant}, mode={mode} — accuracy: {acc:.3f} on {len(preds)} claims.\")\n\ndf_out = pd.DataFrame(rows_out)\nout_path = \"averitec_cot.csv\"\ndf_out.to_csv(out_path, index=False)\nprint(\"\\nSaved detailed results to:\", out_path)\n\nprint(\"\\nSummary (accuracy by variant & mode):\")\nfor mode in PROMPT_MODES:\n    for variant in VARIANTS:\n        vrows = df_out[(df_out[\"variant\"] == variant) & (df_out[\"prompt_mode\"] == mode)]\n        if vrows.empty:\n            continue\n        acc = vrows[\"correct\"].mean()\n        print(f\"  mode={mode:11s} | variant={variant:10s} → accuracy {acc:.3f} on {len(vrows)} claims\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}