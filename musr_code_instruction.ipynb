{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os, re, json, time, ast, requests\nfrom typing import List, Dict, Tuple\n\nimport pandas as pd\nfrom tqdm.auto import tqdm\nfrom functools import lru_cache\n\nimport google.generativeai as genai\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom transformers import logging as hf_logging\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nfrom datasets import load_dataset\n\nuser_secrets = UserSecretsClient()\nHF_TOKEN = user_secrets.get_secret(\"HF_TOKEN\")\nSERPER_API_KEY = user_secrets.get_secret(\"SERPER_API_KEY\")\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\nlogin(token=HF_TOKEN)\n\nmodel_name = \"meta-llama/Llama-3.2-3B-Instruct\"\nDEVICE_MAP = \"auto\"\nllm_name = \"gemini-2.5-flash-lite\"\n\nSPLIT_NAME = \"murder_mysteries\"\nNUM_EXAMPLES = 100\nPRINT_FIRST_N_DEBUG = 3\nSLEEP_BETWEEN_GEMINI_CALLS = 5.0\nWEB_TIMEOUT = 10\nweb_results = 5\n\ngenai.configure(api_key=GEMINI_API_KEY)\ngemini_model = genai.GenerativeModel(llm_name)\nprint(\"Gemini model initialised.\")\n\nprint(\"Loading local SLM (this can take a bit)...\")\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\nslm_tokenizer = AutoTokenizer.from_pretrained(model_name)\nslm_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=DEVICE_MAP,\n    quantization_config=bnb_config,\n)\nhf_logging.set_verbosity_error()\nslm_tokenizer.pad_token = slm_tokenizer.eos_token\nslm_model.config.pad_token_id = slm_tokenizer.pad_token_id\nprint(\"Local SLM loaded.\")\n\n\ndef slm_generate(prompt: str, max_new_tokens: int = 200) -> str:\n    inputs = slm_tokenizer(prompt, return_tensors=\"pt\").to(slm_model.device)\n    out_ids = slm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n    text = slm_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n    return text.strip()\n\n\ndef web_search(query: str, num_results: int = 2):\n    num_results = min(num_results, web_results)\n    results = []\n    if SERPER_API_KEY:\n        url = \"https://google.serper.dev/search\"\n        headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n        payload = {\"q\": query}\n        try:\n            r = requests.post(url, headers=headers, json=payload, timeout=WEB_TIMEOUT)\n            data = r.json()\n            if data.get(\"organic\"):\n                for item in data[\"organic\"][:num_results]:\n                    snippet = (item.get(\"snippet\") or \"\").replace(\"\\n\", \" \")\n                    link = item.get(\"link\") or \"\"\n                    if snippet:\n                        results.append({\"snippet\": snippet, \"url\": link})\n        except Exception as e:\n            print(\"Serper error:\", e)\n    if not results:\n        results.append({\"snippet\": f\"General information about: {query}\", \"url\": \"\"})\n    return results[:num_results]\n\n\n@lru_cache(maxsize=4096)\ndef web_search_cached(q: str, num_results: int = 2):\n    res = web_search(q, num_results=num_results)\n    return tuple((d[\"snippet\"], d[\"url\"]) for d in res)\n\n\ndef call_gemini_with_cooldown(prompt: str, max_retries: int = 3):\n    last_exc = None\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = gemini_model.generate_content(prompt)\n            time.sleep(SLEEP_BETWEEN_GEMINI_CALLS)\n            return resp\n        except Exception as e:\n            last_exc = e\n            msg = str(e)\n            wait_s = None\n            m1 = re.search(r\"retry in ([0-9.]+)s\", msg, flags=re.I)\n            if m1:\n                wait_s = float(m1.group(1))\n            else:\n                m2 = re.search(r\"seconds:\\s*([0-9]+)\", msg, flags=re.I)\n                if m2:\n                    wait_s = float(m2.group(1))\n            if wait_s is None:\n                wait_s = 60.0\n            print(f\"[Gemini rate/HTTP error] Attempt {attempt}/{max_retries} -> sleeping {wait_s:.1f}s...\")\n            time.sleep(wait_s)\n    raise RuntimeError(\"Gemini generate_content failed after retries.\") from last_exc\n\n\ndef _looks_bad(q: str) -> bool:\n    q = (q or \"\").strip().lower()\n    if not q:\n        return True\n    if q in {\"<why>\", \"<what>\", \"<how>\"}:\n        return True\n    if len(re.sub(r\"[^a-z]\", \"\", q)) < 5:\n        return True\n    return False\n\n\ndef _simple_fallback_decomp(full_q: str) -> Dict[str, str]:\n    base = re.sub(r\"\\s*\\?$\", \"\", full_q).strip()\n    if not base:\n        base = full_q.strip()\n    return {\n        \"WHY\": f\"Why is the following question challenging to answer correctly: {base}?\",\n        \"WHAT\": f\"What key facts from the narrative are needed to answer: {base}?\",\n        \"HOW\": f\"How can reasoning over the narrative help answer: {base}?\",\n    }\n\n\ndef slm_decompose_queries(full_question: str) -> Dict[str, str]:\n    prompt = (\n        \"You are a query decomposition assistant for MuSR-style multi-step reasoning tasks.\\n\"\n        \"Given a narrative and a question, rewrite it into exactly three concise sub-queries:\\n\"\n        \"WHY: reasoning or explanation to focus on\\n\"\n        \"WHAT: key entities, events, or definitions needed\\n\"\n        \"HOW: steps or logical process to connect facts\\n\\n\"\n        \"Return EXACTLY:\\nWHY: <text>\\nWHAT: <text>\\nHOW: <text>\\n\\n\"\n        f\"Input (narrative + question): {full_question}\\n\\nDecomposed queries:\"\n    )\n    text = slm_generate(prompt, max_new_tokens=192)\n    why = what = how = \"\"\n    m_why = re.search(r\"WHY:\\s*(.+)\", text, flags=re.I)\n    m_what = re.search(r\"WHAT:\\s*(.+)\", text, flags=re.I)\n    m_how = re.search(r\"HOW:\\s*(.+)\", text, flags=re.I)\n    if m_why:\n        why = m_why.group(1).strip()\n    if m_what:\n        what = m_what.group(1).strip()\n    if m_how:\n        how = m_how.group(1).strip()\n    if _looks_bad(why) or _looks_bad(what) or _looks_bad(how):\n        return _simple_fallback_decomp(full_question)\n    return {\"WHY\": why, \"WHAT\": what, \"HOW\": how}\n\n\ndef slm_build_hints(full_question: str, tagged_snippets: List[Dict[str, str]]) -> List[str]:\n    prompt = (\n        \"You are helping with a MuSR-style multi-step reasoning benchmark.\\n\"\n        \"Given the narrative and question, and web snippets tagged WHY/WHAT/HOW,\\n\"\n        \"extract 4-8 short, verifiable bullet points that help decide the answer.\\n\"\n        \"Do NOT answer the question, and do NOT mention any option letter.\\n\\n\"\n        f\"Narrative + question:\\n{full_question}\\n\\nWeb snippets:\\n\"\n    )\n    for i, s in enumerate(tagged_snippets, 1):\n        prompt += f\"[{i}] {s['snippet']}\\n\"\n    prompt += \"\\nWrite 4-8 bullets. Start each line with '- ' only:\\n\"\n    text = slm_generate(prompt, max_new_tokens=256)\n    facts: List[str] = []\n    for line in text.splitlines():\n        ln = line.strip()\n        if re.match(r\"^(-|\\*|•|\\d+\\.)\\s+\", ln):\n            ln = re.sub(r\"^(\\*|•|\\d+\\.)\\s+\", \"- \", ln)\n        if ln.startswith(\"- \"):\n            fact = ln[2:].strip()\n            if fact:\n                facts.append(fact)\n    if not facts:\n        facts = [s[\"snippet\"] for s in tagged_snippets]\n    return facts[:8]\n\n\ndef build_musr_instruction_prompt(\n    narrative: str,\n    question: str,\n    options_dict: Dict[str, str],\n    hints: List[str] = None,\n) -> str:\n    text = (\n        \"You are solving a multi-step reasoning multiple-choice question from the MuSR benchmark.\\n\"\n        \"You are given a narrative, a question about it, and several answer options.\\n\"\n        \"Your task is to choose the single best answer option.\\n\\n\"\n        \"Instructions:\\n\"\n        \"  i) Carefully read the narrative and understand the key events and relationships.\\n\"\n        \"  ii) Read the question and determine exactly what is being asked.\\n\"\n        \"  iii) Consider the logical consequences of the narrative to evaluate each option.\\n\"\n        \"  iv) You may reason step-by-step INTERNALLY, but you MUST NOT show your reasoning.\\n\"\n        \"  v) Only output the final answer option as a single capital letter (for example A, B, C).\\n\\n\"\n        \"Narrative:\\n\"\n        f\"{narrative}\\n\\n\"\n        f\"Question: {question}\\n\\n\"\n        \"Options:\\n\"\n    )\n    for lab in sorted(options_dict.keys()):\n        text += f\"{lab}. {options_dict[lab]}\\n\"\n    if hints:\n        text += \"\\nOptional factual hints from a smaller helper model (may be incomplete or noisy):\\n\"\n        for h in hints:\n            text += f\"- {h}\\n\"\n    text += \"\\nFinal answer (ONLY write a single letter corresponding to the best option, such as A or B):\"\n    return text\n\n\ndef gemini_musr_answer(narrative, question, options_dict, hints,):\n    prompt = build_musr_instruction_prompt(narrative, question, options_dict, hints=hints)\n    resp = call_gemini_with_cooldown(prompt)\n    out_text = (getattr(resp, \"text\", \"\") or \"\").strip()\n    pattern = r\"\\b[\" + \"\".join(sorted(options_dict.keys())) + r\"]\\b\"\n    m = re.search(pattern, out_text)\n    if m:\n        return m.group(0), out_text\n    m2 = re.search(r\"\\b[A-Z]\\b\", out_text)\n    if m2 and m2.group(0) in options_dict:\n        return m2.group(0), out_text\n    first_key = sorted(options_dict.keys())[0]\n    return first_key, out_text\n\n\ndef parse_choices(raw) -> List[str]:\n    if isinstance(raw, (list, tuple)):\n        return [str(x) for x in raw]\n    if isinstance(raw, str):\n        s = raw.strip()\n        try:\n            val = json.loads(s)\n            if isinstance(val, list):\n                return [str(x) for x in val]\n        except Exception:\n            pass\n        try:\n            val = ast.literal_eval(s)\n            if isinstance(val, (list, tuple)):\n                return [str(x) for x in val]\n        except Exception:\n            pass\n        return [s]\n    return [str(raw)]\n\n\ndef prepare_musr_example(row) -> Tuple[str, str, Dict[str, str], str]:\n    if \"narrative\" not in row or \"question\" not in row or \"choices\" not in row or \"answer_index\" not in row:\n        return None, None, None, None\n    narrative = str(row[\"narrative\"])\n    question = str(row[\"question\"])\n    choices = parse_choices(row[\"choices\"])\n    k = len(choices)\n    if k < 2 or k > 4:\n        return None, None, None, None\n    ans_idx_raw = row[\"answer_index\"]\n    try:\n        ans_idx = int(ans_idx_raw)\n    except Exception:\n        return None, None, None, None\n    if ans_idx < 0 or ans_idx >= k:\n        return None, None, None, None\n    labels = list(\"ABCD\")[:k]\n    options_dict = {labels[i]: choices[i] for i in range(k)}\n    gold = labels[ans_idx]\n    return narrative, question, options_dict, gold\n\n\ndef predict_musr_for_example(row, verbose: bool = False):\n    narrative, question, options_dict, gold = prepare_musr_example(row)\n    if options_dict is None:\n        return None, None, None, [], None\n    full_q = narrative + \"\\n\\nQuestion: \" + question\n    sub_queries: Dict[str, str] = slm_decompose_queries(full_q)\n    tagged = []\n    for tag in [\"WHY\", \"WHAT\", \"HOW\"]:\n        q_sub = sub_queries[tag]\n        for snip, url in web_search_cached(q_sub, num_results=2):\n            tagged.append({\"snippet\": f\"[{tag}] {snip}\", \"url\": url})\n    hints = slm_build_hints(full_q, tagged)\n    pred_label, gemini_raw = gemini_musr_answer(\n        narrative,\n        question,\n        options_dict,\n        hints=hints if hints else None,\n    )\n    return pred_label, gold, sub_queries, hints, gemini_raw\n\n\ndataset = load_dataset(\"TAUR-Lab/MuSR\")\nds = dataset[SPLIT_NAME]\ndf_all = ds.to_pandas()\n\nprint(f\"MuSR split '{SPLIT_NAME}' loaded.\")\nprint(\"Columns:\", list(df_all.columns))\n\nrows_out: List[Dict] = []\npreds: List[str] = []\ngolds: List[str] = []\ncorrect_flags: List[int] = []\n\nprint(\"\\n=== Running MuSR — our_method, instruction ===\")\n\nused = 0\nskipped = 0\n\nfor i in tqdm(range(len(df_all)), desc=\"musr_our_method_instruction\"):\n    if used >= NUM_EXAMPLES:\n        break\n    r = df_all.iloc[i]\n    pred_label, gold, sub_queries, hints, gemini_raw = predict_musr_for_example(\n        r,\n        verbose=(used < PRINT_FIRST_N_DEBUG),\n    )\n    if pred_label is None:\n        skipped += 1\n        continue\n    used += 1\n    preds.append(pred_label)\n    golds.append(gold)\n    correct = int(pred_label == gold)\n    correct_flags.append(correct)\n    rows_out.append({\n        \"idx_in_split\": int(i),\n        \"variant\": \"our_method\",\n        \"split\": SPLIT_NAME,\n        \"prompt_mode\": \"instruction\",\n        \"narrative\": r[\"narrative\"],\n        \"question\": r[\"question\"],\n        \"choices_raw\": json.dumps(parse_choices(r[\"choices\"]), ensure_ascii=False),\n        \"gold_label\": gold,\n        \"pred_label\": pred_label,\n        \"correct\": correct,\n        \"gemini_output\": gemini_raw,\n        \"sub_queries\": json.dumps(sub_queries, ensure_ascii=False) if sub_queries else \"\",\n        \"hints\": json.dumps(hints, ensure_ascii=False) if hints else \"\",\n    })\n    if used > PRINT_FIRST_N_DEBUG:\n        print(f\"Example {used}: gold={gold} | pred={pred_label} | correct={bool(correct)}\")\n\nacc = sum(correct_flags) / max(1, len(preds))\nprint(f\"\\nPipeline accuracy (MuSR, our_method, instruction): {acc:.3f} on {len(preds)} examples.\")\nprint(f\"Skipped examples (invalid choices or answer_index): {skipped}\")\n\ndf_out = pd.DataFrame(rows_out)\nout_path = f\"musr_{SPLIT_NAME}_instruction_our_method.csv\"\ndf_out.to_csv(out_path, index=False)\nprint(\"Saved detailed results to:\", out_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}