{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport re\nimport json\nimport time\nimport requests\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport google.generativeai as genai\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login\n\nuser_secrets = UserSecretsClient()\nlogin(token=user_secrets.get_secret(\"HF_TOKEN\"))\n\nSERPER_API_KEY = user_secrets.get_secret(\"SERPER_API_KEY\")\nGEMINI_API_KEY = user_secrets.get_secret(\"GEMINI_API_KEY\")\n\npath = \"/kaggle/input/gpqa-dataset/gpqa_100.csv\"\n\nmodel_name = \"...\"\nDEVICE_MAP = \"auto\"\nllm_name = \"gemini-2.5-flash-lite\"\n\nweb_results = 5\nweb_timeout = 10\n\ntotal_eval_datapoints = 100\nSLEEP_BETWEEN_GEMINI_CALLS = 5.0\nGEMINI_MAX_RETRIES = 3\n\ngemini_model = None\ngenai.configure(api_key=GEMINI_API_KEY)\ngemini_model = genai.GenerativeModel(llm_name)\nprint(\"Gemini model initialised.\")\n\nprint(\"Loading SLM.\")\nbnb_config = BitsAndBytesConfig(load_in_4bit=True)\nslm_tokenizer = AutoTokenizer.from_pretrained(model_name)\nslm_model = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=DEVICE_MAP,\n    quantization_config=bnb_config,\n)\nprint(\"Local SLM loaded.\")\n\n\ndef slm_generate(prompt: str, max_new_tokens: int = 64) -> str:\n    inputs = slm_tokenizer(prompt, return_tensors=\"pt\").to(slm_model.device)\n    out_ids = slm_model.generate(**inputs, max_new_tokens=max_new_tokens)\n    text = slm_tokenizer.decode(out_ids[0], skip_special_tokens=True)\n    return text.strip()\n\n\ndef web_search(query: str, num_results: int = 2):\n    num_results = min(num_results, web_results)\n    results = []\n\n    if SERPER_API_KEY:\n        url = \"https://google.serper.dev/search\"\n        headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n        payload = {\"q\": query}\n        try:\n            r = requests.post(url, headers=headers, json=payload, timeout=web_timeout)\n            data = r.json()\n            if data.get(\"organic\"):\n                for item in data[\"organic\"][:num_results]:\n                    snippet = (item.get(\"snippet\") or \"\").replace(\"\\n\", \" \")\n                    link = item.get(\"link\") or \"\"\n                    if snippet:\n                        results.append({\"snippet\": snippet, \"url\": link})\n        except Exception as e:\n            print(\"Serper error:\", e)\n\n    if not results:\n        results.append({\"snippet\": f\"General information about: {query}\", \"url\": \"\"})\n\n    return results[:num_results]\n\n\ndef call_gemini_with_cooldown(prompt: str, max_retries: int = GEMINI_MAX_RETRIES):\n    if gemini_model is None:\n        raise RuntimeError(\"Gemini model not initialised.\")\n\n    last_exc = None\n    for attempt in range(1, max_retries + 1):\n        try:\n            resp = gemini_model.generate_content(prompt)\n            time.sleep(SLEEP_BETWEEN_GEMINI_CALLS)\n            return resp\n        except Exception as e:\n            last_exc = e\n            msg = str(e)\n            wait_s = None\n            m1 = re.search(r\"retry in ([0-9.]+)s\", msg, flags=re.I)\n            if m1:\n                wait_s = float(m1.group(1))\n            else:\n                m2 = re.search(r\"seconds:\\s*([0-9]+)\", msg, flags=re.I)\n                if m2:\n                    wait_s = float(m2.group(1))\n            if wait_s is None:\n                wait_s = 60.0\n            print(f\"[Gemini rate/HTTP error] Attempt {attempt}/{max_retries} → sleeping {wait_s:.1f}s...\")\n            time.sleep(wait_s)\n\n    raise RuntimeError(f\"Gemini generate_content failed after {max_retries} retries.\") from last_exc\n\n\ndef slm_decompose_queries(question: str):\n    prompt = (\n        \"You are a query decomposition assistant for advanced physics exam questions.\\n\"\n        \"Given a graduate-level multiple-choice physics question, rewrite it into\\n\"\n        \"exactly three concise sub-questions:\\n\"\n        \"1) a 'WHY' question (asks for reasoning / cause / explanation)\\n\"\n        \"2) a 'WHAT' question (asks for definition / identity / quantity)\\n\"\n        \"3) a 'HOW' question (asks for mechanism, method, or process)\\n\\n\"\n        \"Return them in the following format exactly:\\n\"\n        \"WHY: <why_question>\\n\"\n        \"WHAT: <what_question>\\n\"\n        \"HOW: <how_question>\\n\\n\"\n        f\"Original question: {question}\\n\\n\"\n        \"Decomposed queries:\"\n    )\n\n    text = slm_generate(prompt, max_new_tokens=128)\n\n    why = what = how = question\n    m_why = re.search(r\"WHY:\\s*(.+)\", text, flags=re.IGNORECASE)\n    m_what = re.search(r\"WHAT:\\s*(.+)\", text, flags=re.IGNORECASE)\n    m_how = re.search(r\"HOW:\\s*(.+)\", text, flags=re.IGNORECASE)\n\n    if m_why:\n        why = m_why.group(1).strip()\n    if m_what:\n        what = m_what.group(1).strip()\n    if m_how:\n        how = m_how.group(1).strip()\n\n    return {\"WHY\": why, \"WHAT\": what, \"HOW\": how}\n\n\ndef slm_build_hints(question: str, tagged_snippets):\n    prompt = (\n        \"You are helping with a graduate-level physics multiple-choice exam.\\n\"\n        \"You are given one physics question and some web snippets that were retrieved\\n\"\n        \"using different sub-queries (WHY / WHAT / HOW).\\n\"\n        \"Your job is to extract 4–8 short factual points that are helpful for reasoning\\n\"\n        \"about the correct answer.\\n\"\n        \"Write them as an enumerated list using i), ii), iii), iv), ...\\n\"\n        \"Each point must be on its own line starting with i), ii), etc.\\n\"\n        \"IMPORTANT: Do NOT answer the question yourself. Only write the factual points.\\n\"\n        \"Each point should be a single simple statement about physics or relevant\\n\"\n        \"quantitative relations.\\n\\n\"\n        f\"Question: {question}\\n\\n\"\n        \"Web snippets (with tags):\\n\"\n    )\n    for i, s in enumerate(tagged_snippets, 1):\n        prompt += f\"[{i}] {s['snippet']}\\n\"\n\n    prompt += (\n        \"\\nNow write 4–8 factual points, each on a new line, starting with i), ii), iii), etc.\\n\"\n        \"Do not include anything else.\\n\"\n    )\n\n    text = slm_generate(prompt, max_new_tokens=192)\n\n    facts = []\n    for line in text.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        m = re.match(r\"(?i)^([ivx]+)\\)\\s*(.+)\", line)\n        if m:\n            fact = m.group(2).strip()\n        elif line.startswith(\"-\"):\n            fact = line.lstrip(\"-\").strip()\n        else:\n            continue\n        if fact:\n            facts.append(fact)\n\n    if not facts:\n        facts = [s[\"snippet\"] for s in tagged_snippets]\n\n    return facts\n\n\ndef build_gpqa_instruction_prompt(question, options_dict, facts):\n    text = (\n        \"You are solving a graduate-level physics multiple-choice question from a\\n\"\n        \"challenging exam (similar to GPQA).\\n\"\n        \"Your goal is to choose the single best answer among A, B, C, and D.\\n\\n\"\n        \"Instructions:\\n\"\n        \"  i) Carefully read the question and interpret any symbols, units, or context.\\n\"\n        \"  ii) Use your physics knowledge (conceptual and quantitative) to understand\\n\"\n        \"    what is being asked.\\n\"\n        \"  iii) You may reason step-by-step INTERNALLY, but you MUST NOT show your reasoning.\\n\"\n        \"  iv) Use any optional hints only if they seem consistent and helpful; otherwise\\n\"\n        \"    ignore them.\\n\"\n        \"  v) Return ONLY the final answer as a single capital letter: A, B, C, or D.\\n\\n\"\n        f\"Question: {question}\\n\\n\"\n        \"Options:\\n\"\n        f\"A. {options_dict['A']}\\n\"\n        f\"B. {options_dict['B']}\\n\"\n        f\"C. {options_dict['C']}\\n\"\n        f\"D. {options_dict['D']}\\n\\n\"\n    )\n\n    if facts:\n        roman = [\"i\", \"ii\", \"iii\", \"iv\", \"v\", \"vi\", \"vii\", \"viii\"]\n        text += \"Optional factual hints (from a smaller helper model; may ignore if unhelpful):\\n\"\n        for idx, f in enumerate(facts):\n            prefix = roman[idx] if idx < len(roman) else str(idx + 1)\n            text += f\"{prefix}) {f}\\n\"\n    else:\n        text += \"Optional factual hints:\\n\"\n        text += \"i) (none provided; rely entirely on your own knowledge)\\n\"\n\n    text += \"\\nFinal answer (ONLY one letter A, B, C, or D):\"\n    return text\n\n\ndef gemini_mcq_answer(question, options_dict, facts):\n    if gemini_model is None:\n        return \"A\"\n\n    prompt = build_gpqa_instruction_prompt(question, options_dict, facts)\n\n    try:\n        resp = call_gemini_with_cooldown(prompt)\n        out_text = (getattr(resp, \"text\", \"\") or \"\").strip()\n    except Exception as e:\n        print(\"Gemini error while answering GPQA MCQ after retries:\", e)\n        out_text = \"\"\n\n    m = re.search(r\"[ABCD]\", out_text)\n    if m:\n        return m.group(0)\n    return \"A\"\n\n\ndef predict_mcq_for_row(row, verbose: bool = False):\n    question = str(row[\"query\"])\n    options = {\n        \"A\": str(row[\"option a\"]),\n        \"B\": str(row[\"option b\"]),\n        \"C\": str(row[\"option c\"]),\n        \"D\": str(row[\"option d\"]),\n    }\n\n    sub_queries = slm_decompose_queries(question)\n    tagged_snippets = []\n\n    for tag in [\"WHY\", \"WHAT\", \"HOW\"]:\n        subq = sub_queries[tag]\n        sub_snips = web_search(subq, num_results=2)\n        for s in sub_snips:\n            tagged_snippets.append(\n                {\n                    \"snippet\": f\"[{tag}] {s['snippet']}\",\n                    \"url\": s[\"url\"],\n                }\n            )\n\n    facts = slm_build_hints(question, tagged_snippets)\n    ans_llm = gemini_mcq_answer(question, options, facts)\n    final_pred = ans_llm\n\n    if verbose:\n        print(\"\\n\" + \"=\" * 80)\n        print(\"Variant: our_method | Prompt: instruction\")\n        print(f\"Question: {question}\\n\")\n        print(\"Options:\")\n        for lab in [\"A\", \"B\", \"C\", \"D\"]:\n            print(f\"{lab}. {options[lab]}\")\n        print(\"\\nDecomposed sub-queries:\")\n        for tag in [\"WHY\", \"WHAT\", \"HOW\"]:\n            print(f\"  {tag}: {sub_queries[tag]}\")\n        print(\"\\nTagged web snippets:\")\n        for i, s in enumerate(tagged_snippets, 1):\n            print(f\"  [{i}] {s['snippet'][:160]}\")\n        print(\"\\nSLM factual points (i), ii), ... style):\")\n        for f in facts:\n            print(f\"  - {f}\")\n        print(\"\\nLLM (Gemini) final answer:\", final_pred)\n\n    return final_pred, ans_llm, sub_queries, facts\n\n\ndf = pd.read_csv(path)\n\nprint(\"Columns:\", list(df.columns))\n\ntotal = min(total_eval_datapoints, len(df))\npredictions = []\ngold_labels = []\ncorrect_flags = []\nllm_answers = []\nall_sub_queries = []\nall_facts = []\n\nfor idx in tqdm(range(total), desc=\"GPQA-100 (our_method, instruction)\"):\n    row = df.iloc[idx]\n    gold = str(row[\"answer\"]).strip().upper()\n    gold_labels.append(gold)\n\n    verbose = idx < PRINT_FIRST_N_DEBUG\n\n    pred, ans_llm, sub_queries, facts = predict_mcq_for_row(row, verbose=verbose)\n\n    predictions.append(pred)\n    llm_answers.append(ans_llm)\n    all_sub_queries.append(sub_queries)\n    all_facts.append(facts)\n\n    is_corr = int(pred == gold)\n    correct_flags.append(is_corr)\n\n    if not verbose:\n        print(f\"Q{idx + 1}: gold={gold}, pred={pred}, LLM={ans_llm}\")\n\ncorrect = sum(correct_flags)\naccuracy = correct / total\nprint(f\"\\nFinished {total} GPQA questions.\")\nprint(f\"Pipeline accuracy (our_method, instruction): {accuracy:.3f}\")\n\neval_df = df.iloc[:total].copy()\neval_df[\"gold\"] = gold_labels\neval_df[\"pred\"] = predictions\neval_df[\"correct\"] = correct_flags\neval_df[\"llm_answer\"] = llm_answers\neval_df[\"sub_queries\"] = [json.dumps(q) for q in all_sub_queries]\neval_df[\"facts\"] = [json.dumps(f) for f in all_facts]\n\nsave_path = \"gpqa_instruction.csv\"\neval_df.to_csv(save_path, index=False)\nprint(f\"Saved detailed results to: {save_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}